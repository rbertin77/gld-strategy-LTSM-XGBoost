{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bff131c-0951-42ec-86ae-6963776f9a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration from: C:\\Projetos_Python\\gld_lstm_strategy\\configs\\config.yaml\n",
      "Configuration loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup Project Path ---\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to the Python path to allow imports from 'src'\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# --- General Imports ---\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "# --- Custom Project Imports ---\n",
    "from src.training_pipeline import TrainingPipeline\n",
    "from src.backtester import VectorizedBacktester\n",
    "\n",
    "# --- Load Configuration ---\n",
    "config_path = os.path.join(project_root, 'configs', 'config.yaml')\n",
    "print(f\"Loading configuration from: {config_path}\")\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "print(\"Configuration loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13a1f00b-973a-402d-8198-4ac36b846144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seeds set to 2025 for reproducibility.\n",
      "==================================================\n",
      "RUNNING PIPELINE FOR LSTM MODEL\n",
      "==================================================\n",
      "\n",
      "\n",
      "===== Starting STATIC Test Run for model_type='lstm' =====\n",
      "===== Step 1: Loading and Preparing Full Dataset =====\n",
      "--- Loading Main Asset Data ---\n",
      "Loading GLD data from local cache: C:\\Projetos_Python\\gld_lstm_strategy\\data\\gld_data.csv\n",
      "\n",
      "--- Loading Macroeconomic Data ---\n",
      "Loading DX-Y.NYB data from local cache: C:\\Projetos_Python\\gld_lstm_strategy\\data\\dx-y.nyb_data.csv\n",
      "Loading ^TNX data from local cache: C:\\Projetos_Python\\gld_lstm_strategy\\data\\^tnx_data.csv\n",
      "Loading ^VIX data from local cache: C:\\Projetos_Python\\gld_lstm_strategy\\data\\^vix_data.csv\n",
      "Loading CL=F data from local cache: C:\\Projetos_Python\\gld_lstm_strategy\\data\\cl=f_data.csv\n",
      "Loading SI=F data from local cache: C:\\Projetos_Python\\gld_lstm_strategy\\data\\si=f_data.csv\n",
      "Loading TIP data from local cache: C:\\Projetos_Python\\gld_lstm_strategy\\data\\tip_data.csv\n",
      "Loading HG=F data from local cache: C:\\Projetos_Python\\gld_lstm_strategy\\data\\hg=f_data.csv\n",
      "\n",
      "===== Starting Feature Engineering Pipeline =====\n",
      "Step 1: Creating custom OHLCV features...\n",
      "Step 2: Applying 'All' technical indicator strategy from pandas_ta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130it [00:28,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Dropped 2 redundant TA columns.\n",
      "Step 3: Creating custom interaction and ratio features...\n",
      "Step 4: Creating lagged and momentum features...\n",
      "Step 5: Merging macroeconomic features...\n",
      " -> Macro features merged and forward-filled.\n",
      "Step 6: Defining target variable...\n",
      "\n",
      "Pipeline complete. Dropped 77 rows with NaN values.\n",
      "Final dataset shape: (2438, 247)\n",
      "===============================================\n",
      "\n",
      "\n",
      "--- Splitting data chronologically ---\n",
      "Train set size: 1761, Validation set size: 311, Test set size: 366\n",
      "\n",
      "--- Running Feature Selection ---\n",
      "Selected 37 features via BorutaPy.\n",
      "\n",
      "--- Scaling data ---\n",
      "\n",
      "--- Preparing data and training LSTM Model ---\n",
      "Epoch 1/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 199ms/step - accuracy: 0.5331 - loss: 0.6951 - val_accuracy: 0.4180 - val_loss: 0.6958\n",
      "Epoch 2/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 274ms/step - accuracy: 0.5675 - loss: 0.6833 - val_accuracy: 0.4180 - val_loss: 0.6996\n",
      "Epoch 3/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 172ms/step - accuracy: 0.5827 - loss: 0.6806 - val_accuracy: 0.4672 - val_loss: 0.6933\n",
      "Epoch 4/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 192ms/step - accuracy: 0.5580 - loss: 0.6794 - val_accuracy: 0.5574 - val_loss: 0.6876\n",
      "Epoch 5/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 252ms/step - accuracy: 0.5967 - loss: 0.6736 - val_accuracy: 0.5492 - val_loss: 0.6914\n",
      "Epoch 6/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 275ms/step - accuracy: 0.6004 - loss: 0.6664 - val_accuracy: 0.5246 - val_loss: 0.6941\n",
      "Epoch 7/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 201ms/step - accuracy: 0.6265 - loss: 0.6587 - val_accuracy: 0.4180 - val_loss: 0.7245\n",
      "Epoch 8/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 211ms/step - accuracy: 0.6193 - loss: 0.6571 - val_accuracy: 0.5164 - val_loss: 0.6983\n",
      "Epoch 9/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 212ms/step - accuracy: 0.6298 - loss: 0.6499 - val_accuracy: 0.4754 - val_loss: 0.7105\n",
      "Epoch 10/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.6254 - loss: 0.6406 - val_accuracy: 0.6148 - val_loss: 0.6615\n",
      "Epoch 11/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 218ms/step - accuracy: 0.6477 - loss: 0.6402 - val_accuracy: 0.5902 - val_loss: 0.6691\n",
      "Epoch 12/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 199ms/step - accuracy: 0.6618 - loss: 0.6244 - val_accuracy: 0.6311 - val_loss: 0.6769\n",
      "Epoch 13/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 191ms/step - accuracy: 0.6760 - loss: 0.6178 - val_accuracy: 0.6066 - val_loss: 0.6636\n",
      "Epoch 14/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 177ms/step - accuracy: 0.6772 - loss: 0.6096 - val_accuracy: 0.6475 - val_loss: 0.6505\n",
      "Epoch 15/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 184ms/step - accuracy: 0.6897 - loss: 0.5968 - val_accuracy: 0.6475 - val_loss: 0.6280\n",
      "Epoch 16/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 186ms/step - accuracy: 0.7272 - loss: 0.5670 - val_accuracy: 0.6311 - val_loss: 0.6591\n",
      "Epoch 17/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 231ms/step - accuracy: 0.7136 - loss: 0.5656 - val_accuracy: 0.6967 - val_loss: 0.6027\n",
      "Epoch 18/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 193ms/step - accuracy: 0.7325 - loss: 0.5585 - val_accuracy: 0.6639 - val_loss: 0.6081\n",
      "Epoch 19/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 177ms/step - accuracy: 0.7217 - loss: 0.5504 - val_accuracy: 0.7213 - val_loss: 0.5818\n",
      "Epoch 20/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 177ms/step - accuracy: 0.7039 - loss: 0.5451 - val_accuracy: 0.6639 - val_loss: 0.5911\n",
      "Epoch 21/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 185ms/step - accuracy: 0.7466 - loss: 0.5182 - val_accuracy: 0.6148 - val_loss: 0.6094\n",
      "Epoch 22/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 176ms/step - accuracy: 0.7416 - loss: 0.5171 - val_accuracy: 0.6230 - val_loss: 0.6075\n",
      "Epoch 23/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 216ms/step - accuracy: 0.7505 - loss: 0.5106 - val_accuracy: 0.6393 - val_loss: 0.6250\n",
      "Epoch 24/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 181ms/step - accuracy: 0.7580 - loss: 0.4992 - val_accuracy: 0.6230 - val_loss: 0.6431\n",
      "Epoch 25/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 212ms/step - accuracy: 0.7593 - loss: 0.5009 - val_accuracy: 0.6721 - val_loss: 0.6141\n",
      "Epoch 26/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 273ms/step - accuracy: 0.7524 - loss: 0.4962 - val_accuracy: 0.6967 - val_loss: 0.5778\n",
      "Epoch 27/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 209ms/step - accuracy: 0.7547 - loss: 0.4825 - val_accuracy: 0.7377 - val_loss: 0.5516\n",
      "Epoch 28/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 209ms/step - accuracy: 0.7694 - loss: 0.4776 - val_accuracy: 0.6967 - val_loss: 0.5950\n",
      "Epoch 29/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 192ms/step - accuracy: 0.7808 - loss: 0.4649 - val_accuracy: 0.6885 - val_loss: 0.5929\n",
      "Epoch 30/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 194ms/step - accuracy: 0.7784 - loss: 0.4468 - val_accuracy: 0.6803 - val_loss: 0.5862\n",
      "Epoch 31/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 240ms/step - accuracy: 0.7726 - loss: 0.4586 - val_accuracy: 0.7459 - val_loss: 0.5661\n",
      "Epoch 32/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 165ms/step - accuracy: 0.7848 - loss: 0.4408 - val_accuracy: 0.7459 - val_loss: 0.5207\n",
      "Epoch 33/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 190ms/step - accuracy: 0.7871 - loss: 0.4369 - val_accuracy: 0.7459 - val_loss: 0.5330\n",
      "Epoch 34/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 185ms/step - accuracy: 0.7981 - loss: 0.4306 - val_accuracy: 0.7541 - val_loss: 0.5290\n",
      "Epoch 35/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 221ms/step - accuracy: 0.8145 - loss: 0.4210 - val_accuracy: 0.7705 - val_loss: 0.5160\n",
      "Epoch 36/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 185ms/step - accuracy: 0.8110 - loss: 0.4085 - val_accuracy: 0.7213 - val_loss: 0.5375\n",
      "Epoch 37/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 179ms/step - accuracy: 0.8293 - loss: 0.3997 - val_accuracy: 0.7459 - val_loss: 0.5273\n",
      "Epoch 38/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 232ms/step - accuracy: 0.8248 - loss: 0.3864 - val_accuracy: 0.7951 - val_loss: 0.5402\n",
      "Epoch 39/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 184ms/step - accuracy: 0.8274 - loss: 0.3756 - val_accuracy: 0.7541 - val_loss: 0.5484\n",
      "Epoch 40/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 180ms/step - accuracy: 0.8278 - loss: 0.3874 - val_accuracy: 0.7541 - val_loss: 0.5439\n",
      "Epoch 41/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 190ms/step - accuracy: 0.8278 - loss: 0.3773 - val_accuracy: 0.7295 - val_loss: 0.5739\n",
      "Epoch 42/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 189ms/step - accuracy: 0.8375 - loss: 0.3780 - val_accuracy: 0.7131 - val_loss: 0.6157\n",
      "Epoch 43/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 210ms/step - accuracy: 0.8322 - loss: 0.3877 - val_accuracy: 0.6803 - val_loss: 0.6288\n",
      "Epoch 44/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 183ms/step - accuracy: 0.8174 - loss: 0.4004 - val_accuracy: 0.6885 - val_loss: 0.5927\n",
      "Epoch 45/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 181ms/step - accuracy: 0.8323 - loss: 0.3787 - val_accuracy: 0.6885 - val_loss: 0.6083\n",
      "Epoch 46/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 208ms/step - accuracy: 0.8407 - loss: 0.3581 - val_accuracy: 0.6803 - val_loss: 0.6631\n",
      "Epoch 47/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 192ms/step - accuracy: 0.8371 - loss: 0.3507 - val_accuracy: 0.6721 - val_loss: 0.6306\n",
      "Epoch 48/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 187ms/step - accuracy: 0.8363 - loss: 0.3469 - val_accuracy: 0.6475 - val_loss: 0.6834\n",
      "Epoch 49/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 183ms/step - accuracy: 0.8338 - loss: 0.3449 - val_accuracy: 0.6885 - val_loss: 0.6109\n",
      "Epoch 50/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 178ms/step - accuracy: 0.8415 - loss: 0.3769 - val_accuracy: 0.6148 - val_loss: 0.6658\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 135ms/step\n",
      "\n",
      "--- Evaluating Model on Unseen Test Data ---\n",
      "\n",
      "\n",
      "✅ --- LSTM Pipeline Finished! --- ✅\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Run Pipeline with LSTM Model ---\n",
    "\n",
    "# Instantiate the main pipeline\n",
    "pipeline = TrainingPipeline(config=config, project_root=project_root)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"RUNNING PIPELINE FOR LSTM MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Run the static test specifically for the 'lstm' model type\n",
    "lstm_results = pipeline.run_static_test(model_type='lstm')\n",
    "\n",
    "print(\"\\n\\n✅ --- LSTM Pipeline Finished! --- ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74457101-52be-4ff3-8fa0-e338bc710bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "RUNNING PIPELINE FOR XGBOOST MODEL\n",
      "==================================================\n",
      "\n",
      "\n",
      "===== Starting STATIC Test Run for model_type='xgboost' =====\n",
      "===== Step 1: Loading and Preparing Full Dataset =====\n",
      "--- Loading Main Asset Data ---\n",
      "Loading GLD data from local cache: C:\\Projetos_Python\\gld_lstm_strategy\\data\\gld_data.csv\n",
      "\n",
      "--- Loading Macroeconomic Data ---\n",
      "Loading DX-Y.NYB data from local cache: C:\\Projetos_Python\\gld_lstm_strategy\\data\\dx-y.nyb_data.csv\n",
      "Loading ^TNX data from local cache: C:\\Projetos_Python\\gld_lstm_strategy\\data\\^tnx_data.csv\n",
      "Loading ^VIX data from local cache: C:\\Projetos_Python\\gld_lstm_strategy\\data\\^vix_data.csv\n",
      "Loading CL=F data from local cache: C:\\Projetos_Python\\gld_lstm_strategy\\data\\cl=f_data.csv\n",
      "Loading SI=F data from local cache: C:\\Projetos_Python\\gld_lstm_strategy\\data\\si=f_data.csv\n",
      "Loading TIP data from local cache: C:\\Projetos_Python\\gld_lstm_strategy\\data\\tip_data.csv\n",
      "Loading HG=F data from local cache: C:\\Projetos_Python\\gld_lstm_strategy\\data\\hg=f_data.csv\n",
      "\n",
      "===== Starting Feature Engineering Pipeline =====\n",
      "Step 1: Creating custom OHLCV features...\n",
      "Step 2: Applying 'All' technical indicator strategy from pandas_ta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130it [00:15,  8.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Dropped 2 redundant TA columns.\n",
      "Step 3: Creating custom interaction and ratio features...\n",
      "Step 4: Creating lagged and momentum features...\n",
      "Step 5: Merging macroeconomic features...\n",
      " -> Macro features merged and forward-filled.\n",
      "Step 6: Defining target variable...\n",
      "\n",
      "Pipeline complete. Dropped 77 rows with NaN values.\n",
      "Final dataset shape: (2438, 247)\n",
      "===============================================\n",
      "\n",
      "\n",
      "--- Splitting data chronologically ---\n",
      "Train set size: 1761, Validation set size: 311, Test set size: 366\n",
      "\n",
      "--- Running Feature Selection ---\n",
      "Selected 37 features via BorutaPy.\n",
      "\n",
      "--- Scaling data ---\n",
      "\n",
      "--- Preparing data and training XGBoost Model ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Run the static test specifically for the 'xgboost' model type\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m xgboost_results \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_static_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxgboost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ --- XGBoost Pipeline Finished! --- ✅\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Projetos_Python\\gld_lstm_strategy\\src\\training_pipeline.py:127\u001b[0m, in \u001b[0;36mTrainingPipeline.run_static_test\u001b[1;34m(self, model_type)\u001b[0m\n\u001b[0;32m    124\u001b[0m model \u001b[38;5;241m=\u001b[39m build_xgboost_model(random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSEED\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# XGBoost uses Early Stopping via fit parameters\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_final\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    132\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# XGBoost doesn't have a Keras-like history object\u001b[39;00m\n\u001b[0;32m    135\u001b[0m y_pred_proba_test \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test_final)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\.venv\\lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "# --- 2. Run Pipeline with XGBoost Model ---\n",
    "\n",
    "# We can reuse the same pipeline instance\n",
    "print(\"=\"*50)\n",
    "print(\"RUNNING PIPELINE FOR XGBOOST MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Run the static test specifically for the 'xgboost' model type\n",
    "xgboost_results = pipeline.run_static_test(model_type='xgboost')\n",
    "\n",
    "print(\"\\n\\n✅ --- XGBoost Pipeline Finished! --- ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f820960-8502-4ab6-a458-9a2acd9451aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Run Backtests for Both Models ---\n",
    "\n",
    "def run_backtest_for_results(results_dict, config, model_name):\n",
    "    \"\"\"\n",
    "    Helper function to run the backtest for a given results dictionary.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"RUNNING BACKTEST FOR {model_name.upper()} MODEL\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Extract necessary data from the results dictionary\n",
    "    processed_data = results_dict['processed_data_for_backtest']\n",
    "    y_pred_proba = results_dict['pred_probas']\n",
    "    \n",
    "    # Recreate X_test to get the prices and the correct date index\n",
    "    X = processed_data.drop(columns=[config['TARGET_NAME']])\n",
    "    train_val_size = int(len(X) * (1 - config['TEST_SIZE']))\n",
    "    X_test = X.iloc[train_val_size:]\n",
    "    \n",
    "    # Convert probabilities to binary signals\n",
    "    test_predictions = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    # Create a pandas Series for the signals with the correct date index\n",
    "    if model_name.lower() == 'lstm':\n",
    "        signal_dates = X_test.index[config['TIME_STEPS']:]\n",
    "    else: # XGBoost uses 2D data, so no offset is needed for the index\n",
    "        signal_dates = X_test.index\n",
    "        \n",
    "    signals_series = pd.Series(test_predictions, index=signal_dates, name=\"signal\")\n",
    "    \n",
    "    # Get the price data for the same period\n",
    "    price_data_for_backtest = X_test.loc[signal_dates]\n",
    "\n",
    "    # Instantiate and run the backtester\n",
    "    backtester = VectorizedBacktester(\n",
    "        price_data=price_data_for_backtest,\n",
    "        signals=signals_series,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    portfolio = backtester.run(commission=0.001, slippage=0.001)\n",
    "    return portfolio\n",
    "\n",
    "# Run the backtest for each model's results\n",
    "lstm_portfolio = run_backtest_for_results(lstm_results, config, \"LSTM\")\n",
    "xgboost_portfolio = run_backtest_for_results(xgboost_results, config, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ea2f8-d87a-4018-b893-aff994f2d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Final Results Comparison ---\n",
    "\n",
    "# Extract the statistics from both portfolio objects\n",
    "lstm_stats = lstm_portfolio.stats()\n",
    "xgboost_stats = xgboost_portfolio.stats()\n",
    "\n",
    "# Define the key metrics we want to compare\n",
    "metrics_to_compare = [\n",
    "    'Total Return [%]',\n",
    "    'Benchmark Return [%]',\n",
    "    'Sharpe Ratio',\n",
    "    'Sortino Ratio',\n",
    "    'Max Drawdown [%]',\n",
    "    'Win Rate [%]',\n",
    "    'Profit Factor',\n",
    "    'Total Trades'\n",
    "]\n",
    "\n",
    "# Create a comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'LSTM': lstm_stats[metrics_to_compare],\n",
    "    'XGBoost': xgboost_stats[metrics_to_compare]\n",
    "})\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*50)\n",
    "print(\"MODEL BENCHMARK COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "display(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e0771-9298-4fe8-ba3a-bbc511c18b0a",
   "metadata": {},
   "source": [
    "## 5. Benchmark Conclusion\n",
    "\n",
    "(Escreva sua análise aqui)\n",
    "\n",
    "Ao analisar a tabela de comparação, podemos tirar as seguintes conclusões:\n",
    "* **Retorno Total:** O modelo [LSTM/XGBoost] gerou um retorno maior.\n",
    "* **Retorno Ajustado ao Risco:** O modelo [LSTM/XGBoost] apresentou um Sharpe e Sortino Ratio superior, indicando uma melhor performance para o risco assumido.\n",
    "* **Controle de Risco:** O modelo [LSTM/XGBoost] teve um Max Drawdown menor, demonstrando maior capacidade de preservação de capital.\n",
    "* **Consistência:** O modelo [LSTM/XGBoost] teve uma Taxa de Acerto (Win Rate) maior.\n",
    "\n",
    "**Veredito:** Com base nesses resultados, o modelo **[LSTM/XGBoost]** parece ser a escolha superior para esta estratégia de trading, pois [explique sua razão, ex: \"oferece o melhor balanço entre lucratividade e controle de risco.\"]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
